{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbpro \n",
    "import ray \n",
    "import numpy as np \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "from ray import tune \n",
    "from ray.tune.registry import register_env \n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "from tensortrade.feed.core import NameSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_range(start, end, granularity, product):\n",
    "    delta = timedelta(seconds=granularity)\n",
    "    cur_time = start\n",
    "    data = np.array([], dtype=np.float32).reshape(0,6)\n",
    "    while cur_time < end:\n",
    "        print(cur_time)\n",
    "        cur_segment = public_client.get_product_historic_rates(product, start=cur_time, end=(cur_time + (delta * 300)), granularity=granularity)\n",
    "        #print(len(cur_segment))\n",
    "        cur_time = cur_time + (delta * len(cur_segment))\n",
    "        #print(cur_time)\n",
    "        cur_segment = np.flip(np.array(cur_segment), axis=0)\n",
    "        #print(cur_segment.shape)\n",
    "        data = np.concatenate((data, cur_segment), axis=0)\n",
    "        print(data.shape)\n",
    "        time.sleep(0.34)\n",
    "    return data \n",
    "\n",
    "public_client = cbpro.PublicClient()\n",
    "\n",
    "now = datetime.now() \n",
    "delta = timedelta(days = 120)\n",
    "start = now - delta\n",
    "print(start)\n",
    "\n",
    "granularity = 900 \n",
    "ETH_USD = get_data_range(start, now, granularity, 'ETH-USD')\n",
    "print('done')\n",
    "BTC_USD = get_data_range(start, now, granularity, 'BTC-USD')\n",
    "print('done')\n",
    "ETH_BTC = get_data_range(start, now, granularity, 'ETH-BTC')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_env(config):\n",
    "    #ETH_USD = config['ETH_USD']\n",
    "    #BTC_USD = config['BTC_USD']\n",
    "    #ETH_BTC = config['ETH_BTC']\n",
    "    #ETH_USD, BTC_USD, ETH_BTC\n",
    "    coinbase = Exchange(\"Coinbase\", service=execute_order)(\n",
    "        Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"USD-ETH\"),\n",
    "        Stream.source(BTC_USD[:, 4], dtype=\"float\").rename(\"USD-BTC\"),\n",
    "    )\n",
    "    with NameSpace(\"coinbase\"):\n",
    "        coinbase_streams = [\n",
    "            Stream.source(ETH_USD[:, 0] , dtype=\"float\").rename(\"ETH:date\"),\n",
    "            Stream.source(ETH_USD[:, 1] , dtype=\"float\").rename(\"ETH:open\"),\n",
    "            Stream.source(ETH_USD[:, 2] , dtype=\"float\").rename(\"ETH:high\"),\n",
    "            Stream.source(ETH_USD[:, 3] , dtype=\"float\").rename(\"ETH:low\"),\n",
    "            Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"ETH:close\"),\n",
    "            Stream.source(ETH_USD[:, 5] , dtype=\"float\").rename(\"ETH:volume\"),\n",
    "        \n",
    "            Stream.source(BTC_USD[:, 0] , dtype=\"float\").rename(\"BTC:date\"),\n",
    "            Stream.source(BTC_USD[:, 1] , dtype=\"float\").rename(\"BTC:open\"),\n",
    "            Stream.source(BTC_USD[:, 2] , dtype=\"float\").rename(\"BTC:high\"),\n",
    "            Stream.source(BTC_USD[:, 3] , dtype=\"float\").rename(\"BTC:low\"),\n",
    "            Stream.source(BTC_USD[:, 4] , dtype=\"float\").rename(\"BTC:close\"),\n",
    "            Stream.source(BTC_USD[:, 5] , dtype=\"float\").rename(\"BTC:volume\"),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    feed = DataFeed(coinbase_streams)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        Wallet(coinbase, 3000 * USD),\n",
    "        Wallet(coinbase, 0.01 * BTC),\n",
    "        Wallet(coinbase, 0.3 * ETH),\n",
    "    ])\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(ETH_USD[:, 0] , dtype=\"float\").rename(\"date\"),\n",
    "        #Stream.source(ETH_USD[:, 1] , dtype=\"float\").rename(\"open\"),\n",
    "        #Stream.source(ETH_USD[:, 2] , dtype=\"float\").rename(\"high\"),\n",
    "        #Stream.source(ETH_USD[:, 3] , dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(ETH_USD[:, 5] , dtype=\"float\").rename(\"volume\"),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    from tensortrade.env.default import stoppers\n",
    "\n",
    "    stopper = stoppers.MaxLossStopper(\n",
    "        max_allowed_loss=0.5\n",
    "    )\n",
    "\n",
    "    from tensortrade.env.default.rewards import RiskAdjustedReturns \n",
    "\n",
    "    reward = RiskAdjustedReturns('sortino', window_size = 50)\n",
    "\n",
    "    env = default.create(\n",
    "        portfolio=portfolio,\n",
    "        #action_scheme=\"managed-risk\",\n",
    "        action_scheme=\"simple\",\n",
    "        reward_scheme=reward,\n",
    "        feed=feed,\n",
    "        stopper=stopper,\n",
    "        #renderer_feed=renderer_feed,\n",
    "        #renderer=default.renderers.PlotlyTradingChart(),\n",
    "        window_size=20\n",
    "    )\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(\"TradingEnv\", setup_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = setup_env({\n",
    "        \"ETH_USD\": ETH_USD,\n",
    "        \"BTC_USD\": BTC_USD, \n",
    "        \"ETH_BTC\": ETH_BTC,\n",
    "    })\n",
    "\n",
    "dir(env) \n",
    "action = 2\n",
    "#env.step(action)\n",
    "dir(env)\n",
    "env.action_space\n",
    "env.reset()\n",
    "\n",
    "_, r, _, _ = env.step(1)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.layers import NoisyLayer\n",
    "from ray.rllib.agents.dqn.distributional_q_tf_model import \\\n",
    "    DistributionalQTFModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import tensorflow as tf \n",
    "\n",
    "class CustomDistributionalQModel(DistributionalQTFModel):\n",
    "    \"\"\"Custom model for DQN.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name, **kw):\n",
    "        super(CustomDistributionalQModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name, **kw)\n",
    "        \n",
    "        l1 = model_config['custom_model_config']['l1']\n",
    "        l2 = model_config['custom_model_config']['l2']\n",
    "        prob = model_config['custom_model_config']['prob']\n",
    "        noisy = model_config['custom_model_config']['use_noisy']\n",
    "        self.inputs = tf.keras.layers.Input(obs_space.shape, name=\"observations\")\n",
    "        x = tf.keras.layers.Flatten()(self.inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        if noisy: \n",
    "            x = NoisyLayer(l1)(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.Dense(l1)(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(prob)(x)\n",
    "        \n",
    "        if noisy: \n",
    "            x = NoisyLayer(l2)(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.Dense(l2)(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(prob)(x)\n",
    "\n",
    "\n",
    "        x = tf.keras.layers.Dense(num_outputs, name=\"my_out\")(x)\n",
    "        \n",
    "        self.base_model = tf.keras.Model(self.inputs, x)\n",
    "\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        # Define the core model layers which will be used by the other\n",
    "        # output heads of DistributionalQModel\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            128,\n",
    "            name=\"my_layer1\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0))(self.inputs)\n",
    "        layer_out = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"my_out\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0))(layer_1)\n",
    "        self.base_model = tf.keras.Model(self.inputs, layer_out)\n",
    "        \"\"\"\n",
    "    # Implement the core forward method.\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.base_model(input_dict[\"obs\"])\n",
    "        return model_out, state\n",
    "    \n",
    "ModelCatalog.register_custom_model(\"CustomDistributionalQModel\", CustomDistributionalQModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e3aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.agents import with_common_config\n",
    "\n",
    "\"\"\"\n",
    "\"num_atoms\": 1,\n",
    "\"v_min\": -10.0,\n",
    "\"v_max\": 10.0,\n",
    "# Whether to use noisy network\n",
    "\"noisy\": True,\n",
    "# control the initial value of noisy nets\n",
    "\"sigma0\": 0.5,\n",
    "# Whether to use dueling dqn\n",
    "\"dueling\": True,\n",
    "# Dense-layer setup for each the advantage branch and the value branch\n",
    "# in a dueling architecture.\n",
    "#\"hiddens\": [512, 256, 128],\n",
    "# Whether to use double dqn\n",
    "\"double_q\": True,\n",
    "# N-step Q learning\n",
    "\"n_step\": 5,\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "env_cfg = {\n",
    "        \"ETH_USD\": ETH_USD,\n",
    "        \"BTC_USD\": BTC_USD, \n",
    "        \"ETH_BTC\": ETH_BTC,\n",
    "    }\n",
    "\n",
    "custom_model_cfg = {\"use_noisy\" : tune.choice([True, False]), \n",
    "                    \"l1\" : tune.randint(256, 1025),\n",
    "                    \"l2\" : tune.randint(256, 1025), \n",
    "                    \"prob\" : tune.uniform(0, 1)\n",
    "                   }\n",
    "\n",
    "# with_common_config(\n",
    "DEFAULT_CONFIG = ({\n",
    "    \"env\": \"TradingEnv\",\n",
    "    \"env_config\": env_cfg,\n",
    "\n",
    "    # === Model ===\n",
    "    # Number of atoms for representing the distribution of return. When\n",
    "    # this is greater than 1, distributional Q-learning is used.\n",
    "    # the discrete supports are bounded by v_min and v_max\n",
    "    \"model\": {\n",
    "        \"custom_model\" : \"CustomDistributionalQModel\", \n",
    "        \"custom_model_config\" : custom_model_cfg,\n",
    "    }, \n",
    "    # === Exploration Settings ===\n",
    "    \"exploration_config\": {\n",
    "        # The Exploration class to use.\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        # Config for the Exploration class' constructor:\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.05,\n",
    "        \"epsilon_timesteps\": 2500,  # Timesteps over which to anneal epsilon.\n",
    "\n",
    "        # For soft_q, use:\n",
    "        # \"exploration_config\" = {\n",
    "        #   \"type\": \"SoftQ\"\n",
    "        #   \"temperature\": [float, e.g. 1.0]\n",
    "        # }\n",
    "    },\n",
    "    \"\"\"\n",
    "    # Switch to greedy actions in evaluation workers.\n",
    "    \"evaluation_config\": {\n",
    "        \"explore\": False,\n",
    "    },\n",
    "    \"\"\"\n",
    "    # Minimum env steps to optimize for per train call. This value does\n",
    "    # not affect learning, only the length of iterations.\n",
    "    \"timesteps_per_iteration\": 25,\n",
    "    # Update the target network every `target_network_update_freq` steps.\n",
    "    \"target_network_update_freq\": 25,\n",
    "    # === Replay buffer ===\n",
    "    # Size of the replay buffer. Note that if async_updates is set, then\n",
    "    # each worker will have a replay buffer of this size.\n",
    "    \"buffer_size\": 10000,\n",
    "    \"\"\"\n",
    "    # If True prioritized replay buffer will be used.\n",
    "    \"prioritized_replay\": True,\n",
    "    # Alpha parameter for prioritized replay buffer.\n",
    "    \"prioritized_replay_alpha\": 0.6,\n",
    "    # Beta parameter for sampling from prioritized replay buffer.\n",
    "    \"prioritized_replay_beta\": 0.4,\n",
    "    # Final value of beta (by default, we use constant beta=0.4).\n",
    "    \"final_prioritized_replay_beta\": 0.4,\n",
    "    # Time steps over which the beta parameter is annealed.\n",
    "    \"prioritized_replay_beta_annealing_timesteps\": 20000,\n",
    "    # Epsilon to add to the TD errors when updating priorities.\n",
    "    \"prioritized_replay_eps\": 1e-6,\n",
    "    # Whether to LZ4 compress observations\n",
    "    \"compress_observations\": False,\n",
    "    # Callback to run before learning on a multi-agent batch of experiences.\n",
    "    \"before_learn_on_batch\": None,\n",
    "    # If set, this will fix the ratio of replayed from a buffer and learned on\n",
    "    # timesteps to sampled from an environment and stored in the replay buffer\n",
    "    # timesteps. Otherwise, the replay will proceed at the native ratio\n",
    "    # determined by (train_batch_size / rollout_fragment_length).\n",
    "    \"training_intensity\": None,\n",
    "    \"\"\"\n",
    "    # === Optimization ===\n",
    "    # Learning rate for adam optimizer\n",
    "    \"lr\": tune.loguniform(1e-6, 5e-4), \n",
    "    # Learning rate schedule\n",
    "    \n",
    "    \"\"\"\n",
    "    \"lr_schedule\": None,\n",
    "    # Adam epsilon hyper parameter\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    # If not None, clip gradients during optimization at this value\n",
    "    \"grad_clip\": 40,\n",
    "    # How many steps of the model to sample before learning starts.\n",
    "    \"learning_starts\": 500,\n",
    "    # Update the replay buffer with this many samples at once. Note that\n",
    "    # this setting applies per-worker if num_workers > 1.\n",
    "    \"rollout_fragment_length\": 4,\n",
    "    \"\"\"\n",
    "    # Size of a batch sampled from replay buffer for training. Note that\n",
    "    # if async_updates is set, then each worker returns gradients for a\n",
    "    # batch of this size.\n",
    "    \"train_batch_size\": tune.randint(16, 129),\n",
    "    \"evaluation_num_episodes\" : 30,\n",
    "    \n",
    "    # === Parallelism ===\n",
    "    # Number of workers for collecting samples with. This only makes sense\n",
    "    # to increase if your environment is particularly slow to sample, or if\n",
    "    # you\"re using the Async or Ape-X optimizers.\n",
    "    \"num_workers\": 2,\n",
    "    # Whether to compute priorities on workers.\n",
    "    \"worker_side_prioritization\": False,\n",
    "    # Prevent iterations from going lower than this time span\n",
    "    \"\"\"\n",
    "    \"min_iter_time_s\": 1,\n",
    "    \"\"\"\n",
    "    \"num_gpus\": 2,\n",
    "    #\"num_gpus_per_worker\" : 1, \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = ({\n",
    "\n",
    "    # === Model ===\n",
    "    # Number of atoms for representing the distribution of return. When\n",
    "    # this is greater than 1, distributional Q-learning is used.\n",
    "    # the discrete supports are bounded by v_min and v_max\n",
    "    \"model\": {\n",
    "        \"custom_model\" : \"CustomDistributionalQModel\", \n",
    "        \"custom_model_config\" : custom_model_cfg,\n",
    "    }, \n",
    "    # === Exploration Settings ===\n",
    "    \"exploration_config\": {\n",
    "        # The Exploration class to use.\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        # Config for the Exploration class' constructor:\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": 0.05,\n",
    "        \"epsilon_timesteps\": 2500,  # Timesteps over which to anneal epsilon.\n",
    "\n",
    "        # For soft_q, use:\n",
    "        # \"exploration_config\" = {\n",
    "        #   \"type\": \"SoftQ\"\n",
    "        #   \"temperature\": [float, e.g. 1.0]\n",
    "        # }\n",
    "    },\n",
    "\n",
    "    # Minimum env steps to optimize for per train call. This value does\n",
    "    # not affect learning, only the length of iterations.\n",
    "    \"timesteps_per_iteration\": 25,\n",
    "    # Update the target network every `target_network_update_freq` steps.\n",
    "    \"target_network_update_freq\": 25,\n",
    "    # === Replay buffer ===\n",
    "    # Size of the replay buffer. Note that if async_updates is set, then\n",
    "    # each worker will have a replay buffer of this size.\n",
    "    \"buffer_size\": 10000,\n",
    "\n",
    "    # === Optimization ===\n",
    "    # Learning rate for adam optimizer\n",
    "    \"lr\": tune.loguniform(1e-6, 5e-4), \n",
    "    # Learning rate schedule\n",
    "    \n",
    "    # Size of a batch sampled from replay buffer for training. Note that\n",
    "    # if async_updates is set, then each worker returns gradients for a\n",
    "    # batch of this size.\n",
    "    \"train_batch_size\": tune.randint(16, 129),\n",
    "    \"evaluation_num_episodes\" : 30,\n",
    "    \n",
    "    # === Parallelism ===\n",
    "    # Number of workers for collecting samples with. This only makes sense\n",
    "    # to increase if your environment is particularly slow to sample, or if\n",
    "    # you\"re using the Async or Ape-X optimizers.\n",
    "    \"num_workers\": 2,\n",
    "    # Whether to compute priorities on workers.\n",
    "    \"worker_side_prioritization\": False,\n",
    "    # Prevent iterations from going lower than this time span\n",
    "    \"num_gpus\": 2,\n",
    "    \n",
    "    \"env\": \"TradingEnv\",\n",
    "    #\"env_config\": env_cfg,\n",
    "\n",
    "})\n",
    "\n",
    "DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.config.list_physical_devices('GPU'))\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e15af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "search_alg = HyperOptSearch()\n",
    "\n",
    "stop1 = ray.tune.stopper.TrialPlateauStopper(\"episode_reward_mean\", 0.01, 4, 500)\n",
    "stop_criteria = ray.tune.stopper.MaximumIterationStopper(750)\n",
    "analysis = tune.run(\n",
    "    \"DQN\",\n",
    "    config=DEFAULT_CONFIG, \n",
    "    stop = [stop_criteria, stop1], \n",
    "    num_samples = 1, \n",
    "    metric='episode_reward_mean', \n",
    "    mode='max', \n",
    "    search_alg=search_alg, \n",
    "    local_dir='ray_results', \n",
    "    verbose=3, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf260b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\"\"\"\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acb399",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dir(analysis.trials[0])\n",
    "analysis.best_config\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ed316",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = with_common_config({\n",
    "    \n",
    "    \"env\": \"TradingEnv\",\n",
    "    \"env_config\": env_cfg,\n",
    "\n",
    "    # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "    # required for using GAE).\n",
    "    \"use_critic\": True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE)\n",
    "    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    \"use_gae\": True,\n",
    "    # The GAE (lambda) parameter.\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # Size of batches collected from each worker.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # Number of timesteps collected for each SGD round. This defines the size\n",
    "    # of each SGD epoch.\n",
    "    \"train_batch_size\": 4000,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the\n",
    "    # minibatch size within each epoch.\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    \"shuffle_sequences\": True,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "    # execute per train batch).\n",
    "    \"num_sgd_iter\": 30,\n",
    "    # Stepsize of SGD.\n",
    "    \"lr\": 5e-5,\n",
    "    # Learning rate schedule.\n",
    "    \"lr_schedule\": None,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "    # you set vf_share_layers=True inside your model's config.\n",
    "    \"vf_loss_coeff\": 1.0,\n",
    "    \"model\": {\n",
    "        # Share layers for value function. If you set this to True, it's\n",
    "        # important to tune vf_loss_coeff.\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # Decay schedule for the entropy regularizer.\n",
    "    \"entropy_coeff_schedule\": None,\n",
    "    # PPO clip parameter.\n",
    "    \"clip_param\": 0.3,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 100.0,\n",
    "    # If specified, clip the global norm of gradients by this amount.\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence.\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Which observation filter to apply to the observation.\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "\n",
    "    # Deprecated keys:\n",
    "    # Share layers for value function. If you set this to True, it's important\n",
    "    # to tune vf_loss_coeff.\n",
    "    # Use config.model.vf_share_layers instead.\n",
    "    #\"vf_share_layers\": DEPRECATED_VALUE,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ed308",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_criteria = ray.tune.stopper.MaximumIterationStopper(100)\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=DEFAULT_CONFIG, \n",
    "    stop = stop_criteria, \n",
    "    num_samples = 2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4161c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
