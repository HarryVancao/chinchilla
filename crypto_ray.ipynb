{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f6b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\anaconda3\\envs\\freqtrade-conda\\lib\\site-packages\\ray\\autoscaler\\_private\\cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cbpro \n",
    "import ray \n",
    "import numpy as np \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "from ray import tune \n",
    "from ray.tune.registry import register_env \n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "from tensortrade.feed.core import NameSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1738b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-07 22:46:39.257865\n",
      "2021-01-07 22:46:39.257865\n",
      "(300, 6)\n",
      "2021-01-11 01:46:39.257865\n",
      "(600, 6)\n",
      "2021-01-14 04:46:39.257865\n",
      "(900, 6)\n",
      "2021-01-17 07:46:39.257865\n",
      "(1200, 6)\n",
      "2021-01-20 10:46:39.257865\n",
      "(1500, 6)\n",
      "2021-01-23 13:46:39.257865\n",
      "(1800, 6)\n",
      "2021-01-26 16:46:39.257865\n",
      "(2100, 6)\n",
      "2021-01-29 19:46:39.257865\n",
      "(2400, 6)\n",
      "2021-02-01 22:46:39.257865\n",
      "(2700, 6)\n",
      "2021-02-05 01:46:39.257865\n",
      "(3000, 6)\n",
      "2021-02-08 04:46:39.257865\n",
      "(3300, 6)\n",
      "2021-02-11 07:46:39.257865\n",
      "(3600, 6)\n",
      "2021-02-14 10:46:39.257865\n",
      "(3900, 6)\n",
      "2021-02-17 13:46:39.257865\n",
      "(4200, 6)\n",
      "2021-02-20 16:46:39.257865\n",
      "(4500, 6)\n",
      "2021-02-23 19:46:39.257865\n",
      "(4800, 6)\n",
      "2021-02-26 22:46:39.257865\n",
      "(5100, 6)\n",
      "2021-03-02 01:46:39.257865\n",
      "(5400, 6)\n",
      "2021-03-05 04:46:39.257865\n",
      "(5700, 6)\n",
      "2021-03-08 07:46:39.257865\n",
      "(6000, 6)\n",
      "2021-03-11 10:46:39.257865\n",
      "(6300, 6)\n",
      "2021-03-14 13:46:39.257865\n",
      "(6600, 6)\n",
      "2021-03-17 16:46:39.257865\n",
      "(6900, 6)\n",
      "2021-03-20 19:46:39.257865\n",
      "(7200, 6)\n",
      "2021-03-23 22:46:39.257865\n",
      "(7500, 6)\n",
      "2021-03-27 01:46:39.257865\n",
      "(7800, 6)\n",
      "2021-03-30 04:46:39.257865\n",
      "(8100, 6)\n",
      "2021-04-02 07:46:39.257865\n",
      "(8400, 6)\n",
      "2021-04-05 10:46:39.257865\n",
      "(8700, 6)\n",
      "2021-04-08 13:46:39.257865\n",
      "(9000, 6)\n",
      "2021-04-11 16:46:39.257865\n",
      "(9300, 6)\n",
      "2021-04-14 19:46:39.257865\n",
      "(9600, 6)\n",
      "2021-04-17 22:46:39.257865\n",
      "(9900, 6)\n",
      "2021-04-21 01:46:39.257865\n",
      "(10200, 6)\n",
      "2021-04-24 04:46:39.257865\n",
      "(10500, 6)\n",
      "2021-04-27 07:46:39.257865\n",
      "(10800, 6)\n",
      "2021-04-30 10:46:39.257865\n",
      "(11100, 6)\n",
      "2021-05-03 13:46:39.257865\n",
      "(11400, 6)\n",
      "2021-05-06 16:46:39.257865\n",
      "(11536, 6)\n",
      "done\n",
      "2021-01-07 22:46:39.257865\n",
      "(300, 6)\n",
      "2021-01-11 01:46:39.257865\n",
      "(600, 6)\n",
      "2021-01-14 04:46:39.257865\n",
      "(900, 6)\n",
      "2021-01-17 07:46:39.257865\n",
      "(1200, 6)\n",
      "2021-01-20 10:46:39.257865\n",
      "(1500, 6)\n",
      "2021-01-23 13:46:39.257865\n",
      "(1800, 6)\n",
      "2021-01-26 16:46:39.257865\n",
      "(2100, 6)\n",
      "2021-01-29 19:46:39.257865\n",
      "(2400, 6)\n",
      "2021-02-01 22:46:39.257865\n",
      "(2700, 6)\n",
      "2021-02-05 01:46:39.257865\n",
      "(3000, 6)\n",
      "2021-02-08 04:46:39.257865\n",
      "(3300, 6)\n",
      "2021-02-11 07:46:39.257865\n",
      "(3600, 6)\n",
      "2021-02-14 10:46:39.257865\n",
      "(3900, 6)\n",
      "2021-02-17 13:46:39.257865\n",
      "(4200, 6)\n",
      "2021-02-20 16:46:39.257865\n",
      "(4500, 6)\n",
      "2021-02-23 19:46:39.257865\n",
      "(4800, 6)\n",
      "2021-02-26 22:46:39.257865\n",
      "(5100, 6)\n",
      "2021-03-02 01:46:39.257865\n",
      "(5400, 6)\n",
      "2021-03-05 04:46:39.257865\n",
      "(5700, 6)\n",
      "2021-03-08 07:46:39.257865\n",
      "(6000, 6)\n",
      "2021-03-11 10:46:39.257865\n",
      "(6300, 6)\n",
      "2021-03-14 13:46:39.257865\n",
      "(6600, 6)\n",
      "2021-03-17 16:46:39.257865\n",
      "(6900, 6)\n",
      "2021-03-20 19:46:39.257865\n",
      "(7200, 6)\n",
      "2021-03-23 22:46:39.257865\n",
      "(7500, 6)\n",
      "2021-03-27 01:46:39.257865\n",
      "(7800, 6)\n",
      "2021-03-30 04:46:39.257865\n",
      "(8100, 6)\n",
      "2021-04-02 07:46:39.257865\n",
      "(8400, 6)\n",
      "2021-04-05 10:46:39.257865\n",
      "(8700, 6)\n",
      "2021-04-08 13:46:39.257865\n",
      "(9000, 6)\n",
      "2021-04-11 16:46:39.257865\n",
      "(9300, 6)\n",
      "2021-04-14 19:46:39.257865\n",
      "(9600, 6)\n",
      "2021-04-17 22:46:39.257865\n",
      "(9900, 6)\n",
      "2021-04-21 01:46:39.257865\n",
      "(10200, 6)\n",
      "2021-04-24 04:46:39.257865\n",
      "(10500, 6)\n",
      "2021-04-27 07:46:39.257865\n",
      "(10800, 6)\n",
      "2021-04-30 10:46:39.257865\n",
      "(11100, 6)\n",
      "2021-05-03 13:46:39.257865\n",
      "(11400, 6)\n",
      "2021-05-06 16:46:39.257865\n",
      "(11536, 6)\n",
      "done\n",
      "2021-01-07 22:46:39.257865\n",
      "(300, 6)\n",
      "2021-01-11 01:46:39.257865\n",
      "(600, 6)\n",
      "2021-01-14 04:46:39.257865\n",
      "(900, 6)\n",
      "2021-01-17 07:46:39.257865\n",
      "(1200, 6)\n",
      "2021-01-20 10:46:39.257865\n",
      "(1500, 6)\n",
      "2021-01-23 13:46:39.257865\n",
      "(1800, 6)\n",
      "2021-01-26 16:46:39.257865\n",
      "(2100, 6)\n",
      "2021-01-29 19:46:39.257865\n",
      "(2400, 6)\n",
      "2021-02-01 22:46:39.257865\n",
      "(2700, 6)\n",
      "2021-02-05 01:46:39.257865\n",
      "(3000, 6)\n",
      "2021-02-08 04:46:39.257865\n",
      "(3300, 6)\n",
      "2021-02-11 07:46:39.257865\n",
      "(3600, 6)\n",
      "2021-02-14 10:46:39.257865\n",
      "(3900, 6)\n",
      "2021-02-17 13:46:39.257865\n",
      "(4200, 6)\n",
      "2021-02-20 16:46:39.257865\n",
      "(4500, 6)\n",
      "2021-02-23 19:46:39.257865\n",
      "(4800, 6)\n",
      "2021-02-26 22:46:39.257865\n",
      "(5100, 6)\n",
      "2021-03-02 01:46:39.257865\n",
      "(5400, 6)\n",
      "2021-03-05 04:46:39.257865\n",
      "(5700, 6)\n",
      "2021-03-08 07:46:39.257865\n",
      "(6000, 6)\n",
      "2021-03-11 10:46:39.257865\n",
      "(6300, 6)\n",
      "2021-03-14 13:46:39.257865\n",
      "(6600, 6)\n",
      "2021-03-17 16:46:39.257865\n",
      "(6900, 6)\n",
      "2021-03-20 19:46:39.257865\n",
      "(7200, 6)\n",
      "2021-03-23 22:46:39.257865\n",
      "(7500, 6)\n",
      "2021-03-27 01:46:39.257865\n",
      "(7800, 6)\n",
      "2021-03-30 04:46:39.257865\n",
      "(8100, 6)\n",
      "2021-04-02 07:46:39.257865\n",
      "(8400, 6)\n",
      "2021-04-05 10:46:39.257865\n",
      "(8700, 6)\n",
      "2021-04-08 13:46:39.257865\n",
      "(9000, 6)\n",
      "2021-04-11 16:46:39.257865\n",
      "(9300, 6)\n",
      "2021-04-14 19:46:39.257865\n",
      "(9600, 6)\n",
      "2021-04-17 22:46:39.257865\n",
      "(9900, 6)\n",
      "2021-04-21 01:46:39.257865\n",
      "(10200, 6)\n",
      "2021-04-24 04:46:39.257865\n",
      "(10500, 6)\n",
      "2021-04-27 07:46:39.257865\n",
      "(10800, 6)\n",
      "2021-04-30 10:46:39.257865\n",
      "(11100, 6)\n",
      "2021-05-03 13:46:39.257865\n",
      "(11400, 6)\n",
      "2021-05-06 16:46:39.257865\n",
      "(11536, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_data_range(start, end, granularity, product):\n",
    "    delta = timedelta(seconds=granularity)\n",
    "    cur_time = start\n",
    "    data = np.array([], dtype=np.float32).reshape(0,6)\n",
    "    while cur_time < end:\n",
    "        print(cur_time)\n",
    "        cur_segment = public_client.get_product_historic_rates(product, start=cur_time, end=(cur_time + (delta * 300)), granularity=granularity)\n",
    "        #print(len(cur_segment))\n",
    "        cur_time = cur_time + (delta * len(cur_segment))\n",
    "        #print(cur_time)\n",
    "        cur_segment = np.flip(np.array(cur_segment), axis=0)\n",
    "        #print(cur_segment.shape)\n",
    "        data = np.concatenate((data, cur_segment), axis=0)\n",
    "        print(data.shape)\n",
    "        time.sleep(0.34)\n",
    "    return data \n",
    "\n",
    "public_client = cbpro.PublicClient()\n",
    "\n",
    "now = datetime.now() \n",
    "delta = timedelta(days = 120)\n",
    "start = now - delta\n",
    "print(start)\n",
    "\n",
    "granularity = 900 \n",
    "ETH_USD = get_data_range(start, now, granularity, 'ETH-USD')\n",
    "print('done')\n",
    "BTC_USD = get_data_range(start, now, granularity, 'BTC-USD')\n",
    "print('done')\n",
    "ETH_BTC = get_data_range(start, now, granularity, 'ETH-BTC')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddefcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_env(config):\n",
    "    #ETH_USD = config['ETH_USD']\n",
    "    #BTC_USD = config['BTC_USD']\n",
    "    #ETH_BTC = config['ETH_BTC']\n",
    "    #ETH_USD, BTC_USD, ETH_BTC\n",
    "    coinbase = Exchange(\"Coinbase\", service=execute_order)(\n",
    "        Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"USD-ETH\"),\n",
    "        Stream.source(BTC_USD[:, 4], dtype=\"float\").rename(\"USD-BTC\"),\n",
    "    )\n",
    "    with NameSpace(\"coinbase\"):\n",
    "        coinbase_streams = [\n",
    "            Stream.source(ETH_USD[:, 0] , dtype=\"float\").rename(\"ETH:date\"),\n",
    "            Stream.source(ETH_USD[:, 1] , dtype=\"float\").rename(\"ETH:open\"),\n",
    "            Stream.source(ETH_USD[:, 2] , dtype=\"float\").rename(\"ETH:high\"),\n",
    "            Stream.source(ETH_USD[:, 3] , dtype=\"float\").rename(\"ETH:low\"),\n",
    "            Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"ETH:close\"),\n",
    "            Stream.source(ETH_USD[:, 5] , dtype=\"float\").rename(\"ETH:volume\"),\n",
    "        \n",
    "            Stream.source(BTC_USD[:, 0] , dtype=\"float\").rename(\"BTC:date\"),\n",
    "            Stream.source(BTC_USD[:, 1] , dtype=\"float\").rename(\"BTC:open\"),\n",
    "            Stream.source(BTC_USD[:, 2] , dtype=\"float\").rename(\"BTC:high\"),\n",
    "            Stream.source(BTC_USD[:, 3] , dtype=\"float\").rename(\"BTC:low\"),\n",
    "            Stream.source(BTC_USD[:, 4] , dtype=\"float\").rename(\"BTC:close\"),\n",
    "            Stream.source(BTC_USD[:, 5] , dtype=\"float\").rename(\"BTC:volume\"),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    feed = DataFeed(coinbase_streams)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        Wallet(coinbase, 5000 * USD),\n",
    "        Wallet(coinbase, 0.01 * BTC),\n",
    "        Wallet(coinbase, 0.3 * ETH),\n",
    "    ])\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(ETH_USD[:, 0] , dtype=\"float\").rename(\"date\"),\n",
    "        Stream.source(ETH_USD[:, 4] , dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(ETH_USD[:, 5] , dtype=\"float\").rename(\"volume\"),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    from tensortrade.env.default import stoppers\n",
    "\n",
    "    stopper = stoppers.MaxLossStopper(\n",
    "        max_allowed_loss=0.3\n",
    "    )\n",
    "\n",
    "    from tensortrade.env.default.rewards import RiskAdjustedReturns \n",
    "\n",
    "    reward = RiskAdjustedReturns('sortino', window_size = 100)\n",
    "\n",
    "    env = default.create(\n",
    "        portfolio=portfolio,\n",
    "        #action_scheme=\"managed-risk\",\n",
    "        action_scheme=\"simple\",\n",
    "        reward_scheme=reward,\n",
    "        feed=feed,\n",
    "        stopper=stopper,\n",
    "        #renderer_feed=renderer_feed,\n",
    "        #renderer=default.renderers.PlotlyTradingChart(),\n",
    "        window_size=20\n",
    "    )\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "register_env(\"TradingEnv\", setup_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782b09e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001137379685689127\n"
     ]
    }
   ],
   "source": [
    "env = setup_env({\n",
    "        \"ETH_USD\": ETH_USD,\n",
    "        \"BTC_USD\": BTC_USD, \n",
    "        \"ETH_BTC\": ETH_BTC,\n",
    "    })\n",
    "\n",
    "dir(env) \n",
    "action = 2\n",
    "#env.step(action)\n",
    "dir(env)\n",
    "env.action_space\n",
    "env.reset()\n",
    "\n",
    "_, r, _, _ = env.step(1)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aeb5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.models.tf.layers import NoisyLayer\n",
    "from ray.rllib.agents.dqn.distributional_q_tf_model import \\\n",
    "    DistributionalQTFModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "        \n",
    "def pctFcn(x):\n",
    "    x1 = tf.experimental.numpy.diff(x, axis=1)\n",
    "    x2 = x1 / x[:, 1:, :]\n",
    "    return x2\n",
    "        \n",
    "\n",
    "class CustomDistributionalQModel(DistributionalQTFModel):\n",
    "    \"\"\"Custom model for DQN.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name, **kw):\n",
    "        super(CustomDistributionalQModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name, **kw)\n",
    "        \n",
    "        l1 = model_config['custom_model_config']['l1']\n",
    "        l2 = model_config['custom_model_config']['l2']\n",
    "        prob = model_config['custom_model_config']['prob']\n",
    "        noisy = model_config['custom_model_config']['noisy']\n",
    "        self.inputs = tf.keras.layers.Input(obs_space.shape, name=\"observations\")\n",
    "        x = tf.keras.layers.Lambda(pctFcn)(self.inputs)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        if noisy: \n",
    "            x = NoisyLayer(l1)(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.Dense(l1)(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(prob)(x)\n",
    "        \n",
    "        if noisy: \n",
    "            x = NoisyLayer(l2)(x)\n",
    "        else:\n",
    "            x = tf.keras.layers.Dense(l2)(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(prob)(x)\n",
    "\n",
    "\n",
    "        x = tf.keras.layers.Dense(num_outputs, name=\"my_out\")(x)\n",
    "        \n",
    "        self.base_model = tf.keras.Model(self.inputs, x)\n",
    "\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        # Define the core model layers which will be used by the other\n",
    "        # output heads of DistributionalQModel\n",
    "        self.inputs = tf.keras.layers.Input(\n",
    "            shape=obs_space.shape, name=\"observations\")\n",
    "        layer_1 = tf.keras.layers.Dense(\n",
    "            128,\n",
    "            name=\"my_layer1\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0))(self.inputs)\n",
    "        layer_out = tf.keras.layers.Dense(\n",
    "            num_outputs,\n",
    "            name=\"my_out\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=normc_initializer(1.0))(layer_1)\n",
    "        self.base_model = tf.keras.Model(self.inputs, layer_out)\n",
    "        \"\"\"\n",
    "    # Implement the core forward method.\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out = self.base_model(input_dict[\"obs\"])\n",
    "        return model_out, state\n",
    "    \n",
    "ModelCatalog.register_custom_model(\"CustomDistributionalQModel\", CustomDistributionalQModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc6e3aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.rllib.agents import with_common_config\n",
    "\n",
    "\"\"\"\n",
    "\"num_atoms\": 1,\n",
    "\"v_min\": -10.0,\n",
    "\"v_max\": 10.0,\n",
    "# Whether to use noisy network\n",
    "\"noisy\": True,\n",
    "# control the initial value of noisy nets\n",
    "\"sigma0\": 0.5,\n",
    "# Whether to use dueling dqn\n",
    "\"dueling\": True,\n",
    "# Dense-layer setup for each the advantage branch and the value branch\n",
    "# in a dueling architecture.\n",
    "#\"hiddens\": [512, 256, 128],\n",
    "# Whether to use double dqn\n",
    "\"double_q\": True,\n",
    "# N-step Q learning\n",
    "\"n_step\": 5,\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847a166a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ETH_USD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8fb96c992f7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m env_cfg = {\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[1;34m\"ETH_USD\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mETH_USD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;34m\"BTC_USD\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBTC_USD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m\"ETH_BTC\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mETH_BTC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ETH_USD' is not defined"
     ]
    }
   ],
   "source": [
    "env_cfg = {\n",
    "        \"ETH_USD\": ETH_USD,\n",
    "        \"BTC_USD\": BTC_USD, \n",
    "        \"ETH_BTC\": ETH_BTC,\n",
    "    }\n",
    "\n",
    "custom_model_cfg = {\"noisy\" : False, \n",
    "                    \"l1\" : tune.randint(256, 1025),\n",
    "                    \"l2\" : tune.randint(256, 1025), \n",
    "                    \"prob\" : tune.uniform(0, 1)\n",
    "                   }\n",
    "\n",
    "DEFAULT_CONFIG = ({\n",
    "\n",
    "    # === Model ===\n",
    "    # Number of atoms for representing the distribution of return. When\n",
    "    # this is greater than 1, distributional Q-learning is used.\n",
    "    # the discrete supports are bounded by v_min and v_max\n",
    "    \"n_step\": 3,\n",
    "    \"model\": {\n",
    "        \"custom_model\" : \"CustomDistributionalQModel\", \n",
    "        \"custom_model_config\" : custom_model_cfg,\n",
    "    }, \n",
    "    # === Exploration Settings ===\n",
    "    \"exploration_config\": {\n",
    "        # The Exploration class to use.\n",
    "        \"type\": \"EpsilonGreedy\",\n",
    "        # Config for the Exploration class' constructor:\n",
    "        \"initial_epsilon\": 1.0,\n",
    "        \"final_epsilon\": tune.loguniform(0.05, 0.25),\n",
    "        \"epsilon_timesteps\": 20000,  # Timesteps over which to anneal epsilon.\n",
    "\n",
    "        # For soft_q, use:\n",
    "        # \"exploration_config\" = {\n",
    "        #   \"type\": \"SoftQ\"\n",
    "        #   \"temperature\": [float, e.g. 1.0]\n",
    "        # }\n",
    "    },\n",
    "\n",
    "    # Minimum env steps to optimize for per train call. This value does\n",
    "    # not affect learning, only the length of iterations.\n",
    "    \"timesteps_per_iteration\": 10,\n",
    "    # Update the target network every `target_network_update_freq` steps.\n",
    "    \"target_network_update_freq\": 15,\n",
    "    # === Replay buffer ===\n",
    "    # Size of the replay buffer. Note that if async_updates is set, then\n",
    "    # each worker will have a replay buffer of this size.\n",
    "    \"buffer_size\": 10000,\n",
    "\n",
    "    # === Optimization ===\n",
    "    # Learning rate for adam optimizer\n",
    "    \"lr\": tune.loguniform(1e-6, 5e-4), \n",
    "    # Learning rate schedule\n",
    "    \n",
    "    # Size of a batch sampled from replay buffer for training. Note that\n",
    "    # if async_updates is set, then each worker returns gradients for a\n",
    "    # batch of this size.\n",
    "    \"train_batch_size\": tune.randint(16, 129),\n",
    "    \"evaluation_num_episodes\" : 30,\n",
    "    \n",
    "    # === Parallelism ===\n",
    "    # Number of workers for collecting samples with. This only makes sense\n",
    "    # to increase if your environment is particularly slow to sample, or if\n",
    "    # you\"re using the Async or Ape-X optimizers.\n",
    "    \"num_workers\": 1,\n",
    "    # Whether to compute priorities on workers.\n",
    "    \"worker_side_prioritization\": False,\n",
    "    # Prevent iterations from going lower than this time span\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "\n",
    "    \n",
    "    \"env\": \"TradingEnv\",\n",
    "    #\"env_config\": env_cfg,\n",
    "\n",
    "})\n",
    "\n",
    "DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504a067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.config.list_physical_devices('GPU'))\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31e15af1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/31.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/2 GPUs, 0.0/13.9 GiB heap, 0.0/6.95 GiB objects<br>Current best trial: 5541228a with episode_reward_mean=26.45657664656042 and parameters={'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'train_batch_size': 62, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'CustomDistributionalQModel', 'custom_model_config': {'noisy': False, 'l1': 384, 'l2': 880, 'prob': 0.4285549245440675}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'TradingEnv', 'env_config': {}, 'render_env': False, 'record_env': False, 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 7.048004575614567e-06, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.1979991954771608, 'epsilon_timesteps': 20000}, 'evaluation_interval': None, 'evaluation_num_episodes': 30, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 10, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': True, 'monitor': -1, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'target_network_update_freq': 15, 'buffer_size': 6000, 'replay_sequence_length': 1, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'final_prioritized_replay_beta': 0.4, 'prioritized_replay_beta_annealing_timesteps': 20000, 'prioritized_replay_eps': 1e-06, 'before_learn_on_batch': None, 'training_intensity': None, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'learning_starts': 1000, 'worker_side_prioritization': False}<br>Result logdir: D:\\source\\repos\\chinchilla\\ray_results\\DQN<br>Number of trials: 50/50 (50 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 16:42:55,697\tINFO tune.py:549 -- Total run time: 17509.88 seconds (17509.30 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "search_alg = HyperOptSearch()\n",
    "\n",
    "stop1 = ray.tune.stopper.TrialPlateauStopper(\"episode_reward_mean\", 0.01, 4, 600)\n",
    "stop_criteria = ray.tune.stopper.MaximumIterationStopper(10000)\n",
    "analysis = tune.run(\n",
    "    \"DQN\",\n",
    "    config=DEFAULT_CONFIG,\n",
    "    stop = [stop_criteria, stop1],\n",
    "    num_samples = 50,\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    search_alg=search_alg,\n",
    "    local_dir='ray_results',\n",
    "    verbose=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf260b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\"\"\"\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"framework\": \"torch\",\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env({\n",
    "    \"window_size\": 25\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0452e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_checkpoints',\n",
       " '_configs',\n",
       " '_experiment_dir',\n",
       " '_experiment_state',\n",
       " '_get_trial_paths',\n",
       " '_retrieve_rows',\n",
       " '_trial_dataframes',\n",
       " '_validate_metric',\n",
       " '_validate_mode',\n",
       " 'best_checkpoint',\n",
       " 'best_config',\n",
       " 'best_dataframe',\n",
       " 'best_logdir',\n",
       " 'best_result',\n",
       " 'best_result_df',\n",
       " 'best_trial',\n",
       " 'dataframe',\n",
       " 'default_metric',\n",
       " 'default_mode',\n",
       " 'fetch_trial_dataframes',\n",
       " 'get_all_configs',\n",
       " 'get_best_checkpoint',\n",
       " 'get_best_config',\n",
       " 'get_best_logdir',\n",
       " 'get_best_trial',\n",
       " 'get_trial_checkpoints_paths',\n",
       " 'results',\n",
       " 'results_df',\n",
       " 'runner_data',\n",
       " 'stats',\n",
       " 'trial_dataframes',\n",
       " 'trials']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m C:\\Users\\Harry\\anaconda3\\envs\\freqtrade-conda\\lib\\site-packages\\ray\\autoscaler\\_private\\cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\r\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m   warnings.warn(\r\n"
     ]
    }
   ],
   "source": [
    "dir(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6acb399",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 26.45657664656042,\n",
       " 'episode_reward_min': 26.45657664656042,\n",
       " 'episode_reward_mean': 26.45657664656042,\n",
       " 'episode_len_mean': 11535.0,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [26.45657664656042, 26.45657664656042],\n",
       "  'episode_lengths': [11535, 11535]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.10181697646995616,\n",
       "  'mean_inference_ms': 5.328535632427477,\n",
       "  'mean_action_processing_ms': 0.04809560211472762,\n",
       "  'mean_env_wait_ms': 4.400964247493533,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 29728,\n",
       " 'agent_timesteps_total': 29728,\n",
       " 'timers': {'learn_time_ms': 22.147,\n",
       "  'learn_throughput': 2799.414,\n",
       "  'update_time_ms': 9.402},\n",
       " 'info': {'learner': {'default_policy': {'cur_lr': 7.0480045906151645e-06,\n",
       "    'mean_q': 0.07622642,\n",
       "    'min_q': 0.046284016,\n",
       "    'max_q': 0.08845338,\n",
       "    'mean_td_error': -0.013147724,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 29728,\n",
       "  'num_agent_steps_sampled': 29728,\n",
       "  'num_steps_trained': 445346,\n",
       "  'num_agent_steps_trained': 445346,\n",
       "  'last_target_update_ts': 29720,\n",
       "  'num_target_updates': 1796},\n",
       " 'done': True,\n",
       " 'episodes_total': 2,\n",
       " 'training_iteration': 600,\n",
       " 'experiment_id': '9e92cea25c6a483abd70fda3b734a495',\n",
       " 'date': '2021-05-05_12-14-27',\n",
       " 'timestamp': 1620231267,\n",
       " 'time_this_iter_s': 1.2010042667388916,\n",
       " 'time_total_s': 670.593412399292,\n",
       " 'pid': 3876,\n",
       " 'hostname': 'DESKTOP-3BVVEU8',\n",
       " 'node_ip': '10.0.0.163',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 4,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'train_batch_size': 62,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'num_framestacks': 'auto',\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': 'CustomDistributionalQModel',\n",
       "   'custom_model_config': {'noisy': False,\n",
       "    'l1': 384,\n",
       "    'l2': 880,\n",
       "    'prob': 0.4285549245440675},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   'framestack': True},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'TradingEnv',\n",
       "  'env_config': {},\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 7.048004575614567e-06,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.1979991954771608,\n",
       "   'epsilon_timesteps': 20000},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 30,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 1,\n",
       "  'timesteps_per_iteration': 10,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 1,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  'simple_optimizer': True,\n",
       "  'monitor': -1,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': False,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 1,\n",
       "  'target_network_update_freq': 15,\n",
       "  'buffer_size': 6000,\n",
       "  'replay_sequence_length': 1,\n",
       "  'prioritized_replay': True,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'learning_starts': 1000,\n",
       "  'worker_side_prioritization': False},\n",
       " 'time_since_restore': 670.593412399292,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 600,\n",
       " 'perf': {'cpu_util_percent': 16.2, 'ram_util_percent': 39.9},\n",
       " 'trial_id': '5541228a',\n",
       " 'experiment_tag': '4_buffer_size=6000,evaluation_num_episodes=30,epsilon_timesteps=20000,final_epsilon=0.198,initial_epsilon=1.0,type=EpsilonGreedy,lr=7.048e-06,custom_model=CustomDistributionalQModel,l1=384,l2=880,noisy=False,prob=0.42855,num_cpus_for_driver=1,num_cpus_per_worker=1,num_gpus=1,num_workers=1,target_network_update_freq=15,timesteps_per_iteration=10,train_batch_size=62,worker_side_prioritization=False'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(analysis)\n",
    "#analysis.best_config\n",
    "\n",
    "#analysis.results\n",
    "analysis.best_result\n",
    "ana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ed316",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = with_common_config({\n",
    "    \n",
    "    \"env\": \"TradingEnv\",\n",
    "    \"env_config\": env_cfg,\n",
    "\n",
    "    # Should use a critic as a baseline (otherwise don't use value baseline;\n",
    "    # required for using GAE).\n",
    "    \"use_critic\": True,\n",
    "    # If true, use the Generalized Advantage Estimator (GAE)\n",
    "    # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.\n",
    "    \"use_gae\": True,\n",
    "    # The GAE (lambda) parameter.\n",
    "    \"lambda\": 1.0,\n",
    "    # Initial coefficient for KL divergence.\n",
    "    \"kl_coeff\": 0.2,\n",
    "    # Size of batches collected from each worker.\n",
    "    \"rollout_fragment_length\": 200,\n",
    "    # Number of timesteps collected for each SGD round. This defines the size\n",
    "    # of each SGD epoch.\n",
    "    \"train_batch_size\": 4000,\n",
    "    # Total SGD batch size across all devices for SGD. This defines the\n",
    "    # minibatch size within each epoch.\n",
    "    \"sgd_minibatch_size\": 128,\n",
    "    # Whether to shuffle sequences in the batch when training (recommended).\n",
    "    \"shuffle_sequences\": True,\n",
    "    # Number of SGD iterations in each outer loop (i.e., number of epochs to\n",
    "    # execute per train batch).\n",
    "    \"num_sgd_iter\": 30,\n",
    "    # Stepsize of SGD.\n",
    "    \"lr\": 5e-5,\n",
    "    # Learning rate schedule.\n",
    "    \"lr_schedule\": None,\n",
    "    # Coefficient of the value function loss. IMPORTANT: you must tune this if\n",
    "    # you set vf_share_layers=True inside your model's config.\n",
    "    \"vf_loss_coeff\": 1.0,\n",
    "    \"model\": {\n",
    "        # Share layers for value function. If you set this to True, it's\n",
    "        # important to tune vf_loss_coeff.\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    # Coefficient of the entropy regularizer.\n",
    "    \"entropy_coeff\": 0.0,\n",
    "    # Decay schedule for the entropy regularizer.\n",
    "    \"entropy_coeff_schedule\": None,\n",
    "    # PPO clip parameter.\n",
    "    \"clip_param\": 0.3,\n",
    "    # Clip param for the value function. Note that this is sensitive to the\n",
    "    # scale of the rewards. If your expected V is large, increase this.\n",
    "    \"vf_clip_param\": 100.0,\n",
    "    # If specified, clip the global norm of gradients by this amount.\n",
    "    \"grad_clip\": None,\n",
    "    # Target value for KL divergence.\n",
    "    \"kl_target\": 0.01,\n",
    "    # Whether to rollout \"complete_episodes\" or \"truncate_episodes\".\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    # Which observation filter to apply to the observation.\n",
    "    \"observation_filter\": \"NoFilter\",\n",
    "\n",
    "    # Deprecated keys:\n",
    "    # Share layers for value function. If you set this to True, it's important\n",
    "    # to tune vf_loss_coeff.\n",
    "    # Use config.model.vf_share_layers instead.\n",
    "    #\"vf_share_layers\": DEPRECATED_VALUE,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ed308",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_criteria = ray.tune.stopper.MaximumIterationStopper(100)\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=DEFAULT_CONFIG, \n",
    "    stop = stop_criteria, \n",
    "    num_samples = 2, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4161c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
